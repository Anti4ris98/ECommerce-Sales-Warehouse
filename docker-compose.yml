services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    ports:
      - '2181:2181'
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    ports:
      - '9092:9092'
      - '29092:29092'
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: INTERNAL://0.0.0.0:29092,EXTERNAL://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:29092,EXTERNAL://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    volumes:
      - kafka_data:/var/lib/kafka/data
    depends_on:
      - zookeeper
    healthcheck:
      test: [ "CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:29092" ]
      interval: 10s
      timeout: 10s
      retries: 5

  postgres:
    image: postgres:15
    ports:
      - '5432:5432'
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
      POSTGRES_DB: ecommerce_dw
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U user -d ecommerce_dw" ]
      interval: 10s
      timeout: 5s
      retries: 5

  grafana:
    image: grafana/grafana:latest
    ports:
      - '3000:3000'
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
    depends_on:
      - postgres

  spark-job:
    build:
      context: .
      dockerfile: Dockerfile.spark
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
    depends_on:
      kafka:
        condition: service_healthy
    restart: on-failure
    healthcheck:
      test: [ "CMD-SHELL", "pgrep -f spark-submit || exit 1" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    command: >
      spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 /app/spark_streaming.py

  batch-etl:
    image: python:3.11-slim
    working_dir: /app
    volumes:
      - ./batch_etl.py:/app/batch_etl.py
    environment:
      - PYTHONUNBUFFERED=1
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - DB_HOST=postgres
      - DB_PORT=5432
      - DB_NAME=ecommerce_dw
      - DB_USER=user
      - DB_PASSWORD=password
    depends_on:
      kafka:
        condition: service_healthy
      postgres:
        condition: service_healthy
    healthcheck:
      test: [ "CMD-SHELL", "pgrep -f batch_etl.py || exit 1" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    command: >
      sh -c "pip install kafka-python psycopg2-binary &&
             python batch_etl.py"
    restart: on-failure

  dbt-refresh:
    image: python:3.11-slim
    working_dir: /dbt_project
    volumes:
      - ./dbt_project:/dbt_project
    environment:
      - DBT_REFRESH_INTERVAL=300 # Refresh every 5 minutes (300 seconds)
    depends_on:
      postgres:
        condition: service_healthy
    command: >
      sh -c "apt-get update && apt-get install -y postgresql-client &&
             pip install dbt-postgres &&
             chmod +x /dbt_project/refresh.sh &&
             /dbt_project/refresh.sh"
    restart: on-failure

  producer:
    image: python:3.11-slim
    working_dir: /app
    volumes:
      - ./producer.py:/app/producer.py
    environment:
      - PYTHONUNBUFFERED=1
    depends_on:
      kafka:
        condition: service_healthy
    command: >
      sh -c "pip install kafka-python-ng requests &&
             python producer.py"
    restart: on-failure

volumes:
  zookeeper_data:
  kafka_data:
  postgres_data:
  grafana_data:
